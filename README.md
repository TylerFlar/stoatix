# Stoatix

A lightweight CLI tool for system benchmarking and diagnostics.

## Quickstart

```bash
# Clone the repository
git clone https://github.com/TylerFlar/stoatix.git
cd stoatix

# Install dependencies
uv sync

# Validate a configuration file
uv run stoatix validate examples/stoatix.yaml

# Dry run (validate + write metadata without executing)
uv run stoatix run examples/stoatix.yaml --dry-run

# Run benchmarks
uv run stoatix run examples/stoatix.yaml --out out/

# Run with shuffle (randomized case order)
uv run stoatix run examples/stoatix.yaml --shuffle --seed 12345

# Run with Linux perf stat counters (Linux only)
uv run stoatix run examples/stoatix.yaml --perf-stat
```

## Results

After running benchmarks, results are written to the output directory:

- **`out/session.json`** — Metadata about the run including suite ID, config hash, system info, and git info
- **`out/config.resolved.yml`** — Resolved configuration with all defaults expanded
- **`out/cases.json`** — Expanded cases in execution order
- **`out/results.jsonl`** — One JSON record per benchmark attempt with timing data
- **`out/summary.csv`** — Aggregated statistics per case (generated by default)

## Summary Statistics

By default, `stoatix run` generates `summary.csv` with per-case statistics.

### Record Selection

- **Only `measured` runs** are included (warmups are excluded)
- **Retry handling**: For each (case_id, iteration), the **first successful attempt** (`ok=true`) is used. If all attempts failed, that iteration is counted as failed and excluded from timing stats.

### Outlier Filtering

| Method | Description |
|--------|-------------|
| `iqr` (default) | Per-case IQR filtering: values outside [Q1 - 1.5×IQR, Q3 + 1.5×IQR] are dropped |
| `none` | Keep all successful elapsed times |

Use `--outliers none` or `--outliers iqr` to control this behavior.

### Statistics Definitions

| Statistic | Definition |
|-----------|------------|
| `median_s` | Median of filtered elapsed times |
| `mean_s` | Arithmetic mean of filtered elapsed times |
| `stdev_s` | Sample standard deviation (ddof=1); blank if n < 2 |
| `p95_s` | 95th percentile using linear interpolation between adjacent sorted values |
| `min_s`, `max_s` | Min/max of filtered elapsed times |

### Standalone Summarization

```bash
# Summarize existing results
uv run stoatix summarize out/results.jsonl

# Specify output path and outlier method
uv run stoatix summarize out/results.jsonl --out report.csv --outliers none
```

## Configuration

Create a YAML file with your benchmarks:

```yaml
# Optional defaults applied to all benchmarks
defaults:
  warmups: 2
  runs: 5
  retries: 0
  timeout_s: 60.0
  env:
    MY_VAR: "value"

benchmarks:
  - name: my-benchmark
    command: ["python", "-c", "print('hello')"]
    warmups: 1      # warmup iterations (not measured)
    runs: 5         # measured iterations
    timeout_s: 10.0 # optional timeout in seconds

  # Matrix expansion example
  - name: parameterized-bench
    command: ["python", "-c", "print({n})"]
    matrix:
      n: [100, 1000, 10000]
```

The `command` must be a list of strings (exec form, no shell).

## CLI Reference

```bash
# Show help
uv run stoatix --help

# Show version
uv run stoatix --version

# Validate config without running
uv run stoatix validate <config.yaml>

# Run benchmarks
uv run stoatix run <config.yaml> [OPTIONS]

# Run command options:
#   --out, -o PATH       Output directory (default: out)
#   --shuffle            Shuffle case execution order
#   --no-shuffle         Preserve deterministic order (default)
#   --seed INTEGER       Random seed for shuffling
#   --dry-run            Write metadata files without executing benchmarks
#   --summarize          Generate summary.csv after run (default)
#   --no-summarize       Skip summary generation
#   --outliers TEXT      Outlier filtering: 'iqr' (default) or 'none'
#   --perf-stat          Collect Linux perf stat counters (Linux only)
#   --perf-events TEXT   Comma-separated perf events (has sensible defaults)
#   --perf-strict        Fail if perf unavailable (default: degrade gracefully)

# Summarize existing results
uv run stoatix summarize <results.jsonl> [OPTIONS]

# Summarize command options:
#   --out, -o PATH       Output CSV path (default: summary.csv in same dir)
#   --outliers TEXT      Outlier filtering: 'iqr' (default) or 'none'

# Compare two result files (e.g., main vs PR)
uv run stoatix compare <main.jsonl> <pr.jsonl> [OPTIONS]

# Compare command options:
#   --threshold FLOAT      Classification threshold (default: 0.05)
#   --metric TEXT          median_s | mean_s | p95_s (default: median_s)
#   --sort TEXT            stable | priority (default: priority)
#   --top INTEGER          Max rows in markdown table (default: 50)
#   --json-out PATH        Output JSON path (default: compare.json next to pr)
#   --md-out PATH          Optional markdown output file

# Examples:
uv run stoatix run config.yaml --out results/
uv run stoatix run config.yaml --shuffle --seed 42
uv run stoatix run config.yaml --no-summarize
uv run stoatix run config.yaml --outliers none
uv run stoatix summarize out/results.jsonl --out report.csv
uv run stoatix compare main/results.jsonl pr/results.jsonl --threshold 0.03
```

## Linux perf stat Integration

On Linux, Stoatix can collect hardware performance counters via `perf stat`:

```bash
# Enable perf stat collection
uv run stoatix run config.yaml --perf-stat

# Custom events (default includes cycles, instructions, cache misses, etc.)
uv run stoatix run config.yaml --perf-stat --perf-events "cycles,instructions,LLC-loads,LLC-load-misses"

# Strict mode: fail if perf is unavailable
uv run stoatix run config.yaml --perf-stat --perf-strict
```

**Requirements:**
- Linux with `perf` installed (`linux-tools-generic` on Ubuntu/Debian)
- Appropriate permissions (may need `sudo` or `kernel.perf_event_paranoid` sysctl)

**Behavior:**
- `--perf-stat`: Wraps each command with `perf stat -e <events> -x, --`
- Counters are stored in `perf_counters` field of each result record
- Without `--perf-strict`: gracefully degrades to timing-only if perf unavailable
- With `--perf-strict`: exits with error if perf cannot collect data

## Comparing Results

The `compare` command detects regressions between two benchmark runs (e.g., main branch vs PR).

### Input

Both arguments must be `results.jsonl` files from `stoatix run`. Cases are matched by `(bench_name, case_id)`.

### Classification

Percent change is computed as:

```
pct_change = (pr_metric - main_metric) / main_metric
```

| Classification | Condition |
|----------------|-----------|
| **regressed** | `pct_change > threshold` |
| **improved** | `pct_change < -threshold` |
| **unchanged** | within ±threshold |
| **added** | case exists only in PR |
| **removed** | case exists only in main |

Default threshold is 5% (`--threshold 0.05`). Default metric is `median_s`.

### Needs Attention

Cases are flagged when results may be unreliable:

- **High CV**: `stdev_s / median_s >= 0.05` (coefficient of variation)
- **Long tail**: `p95_s / median_s >= 1.10`
- **Low samples**: fewer than 3 OK iterations in either run

Tune with `--noise-cv`, `--noise-p95-ratio`, `--min-ok`.

### Determinism

Output is fully deterministic for reproducible CI:

- **Stable sort** (`--sort stable`): rows ordered by `(bench_name, case_key, case_id)`
- **Priority sort** (`--sort priority`, default): regressed cases first, then stable order within each classification

Upstream run ordering can be randomized with `stoatix run --shuffle --seed <N>` to reduce measurement bias, but comparison results remain deterministic regardless of execution order.

## License

MIT
